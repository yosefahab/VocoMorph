experiment: "VocoMorphUnetSTFT"
notes: "Neural Vocal Modulator V2.1
  UNet & FiLM (Feature-wise Linear Modulation) layers
  are used to condition an audio on an Embedding Vector.
  Input is STFT, (length * width) must be divisible by 2^number of unet blocks.
  otherwise upsampling won't restore original size.
  "
# ------------------------------------------------------------------------------------------------------------------------------ #
config:
  # ------------------------------------------------------------ #
  seed: 13

  data:
    num_workers: 5
    pin_memory: true
    drop_last: true

    channels: &var_num_channels 1 # mono/stereo
    sample_rate: &var_sample_rate 16000
    n_mels: 80
    n_fft: &var_n_fft 1024
    win_length: &var_win_length 1024
    hop_length: &var_hop_length 256

    # the chunk of audio the model processes at a time
    chunk_size: &var_chunk_size 4096 # 256 ms at 16k
    # max audio length during training
    max_length: 160000 # 10 sec at 16k sr, -1 if you dont want to trim
    overlap: 0

    datalists:
      train:
        batch_size: 32
        path: "librispeech_train_augmented.csv"
      valid:
        batch_size: 32
        path: "librispeech_valid_augmented.csv"
      test:
        batch_size: 1
        path: "librispeech_test_augmented.csv"

    effects:
      [
        "apply_identity_transform",
        "apply_radio_effect",
        "apply_robotic_effect",
        "apply_ghost_effect",
        "apply_demonic_effect",
      ]
  # ------------------------------------------------------------ #
  model:
    num_channels: *var_num_channels
    chunk_size: *var_chunk_size
    overlap: 0
    embedding_dim: 32
    num_effects: 5
    encoder_filters: [32, 64, 128, 256, 512] # 5 levels
    bottleneck_filters: [1024, 1024]
    decoder_filters: [512, 256, 128, 64, 32] # 5 levels
    kernel_size: [3, 3]
    padding: 1

    module_stft:
      n_fft: *var_n_fft
      hop_length: *var_hop_length
      win_length: *var_win_length
      output_length: *var_chunk_size
  # ------------------------------------------------------------ #
  # loss functions
  criterions:
    # - name: "EnergyLoss"
    #   weight: 10.0
    #
    # - name: "MSELoss"
    #   weight: 1.0
    #
    - name: "SISNRLoss"
      weight: 1.0

    - name: "MultiResolutionSTFTLoss"
      weight: 1.0
      params:
        alpha: 1.0
        beta: 0.5
        resolutions: [
            [512, 512, 128], # Moderate frequency resolution, good time resolution
            [1024, 1024, 256], # Good frequency resolution, medium time resolution
            [2048, 2048, 512], # Excellent frequency resolution, poor time resolution
          ]
    #
    # - name: "STFTLoss"
    #   weight: 0.5
    #   params:
    #     alpha: 0.5
    #     beta: 0.5
    #     n_fft: *var_n_fft
    #     hop_length: *var_hop_length
    #     win_length: *var_win_length
    #
    # - name: "VocalModulationLoss"
    #   weight: 0.7
    #   params:
    #     alpha: 0.5
    #     beta: 0.2
    #     gamma: 0.1
    #     delta: 1.0
    #     sample_rate: *var_sample_rate
    #     n_mels: *var_n_mels
    #     n_fft: *var_n_fft
    #     hop_length: *var_hop_length
    #     win_length: *var_win_length
    #
  # ------------------------------------------------------------ #
  optimizers:
    - name: "AdamW"
      params:
        lr: 5.0e-5
        betas: [0.9, 0.999]
        weight_decay: 1.0e-5
      scheduler:
        name: "ReduceLROnPlateau"
        params:
          mode: "min"
          min_lr: 1.0e-10
          factor: 0.8
          patience: 2
        # name: "WarmupConstantSchedule"
        #   params:
        #     warmup_steps: 1000
  # ------------------------------------------------------------ #
  # metrics to evaluate the model on
  metrics: []
  # ------------------------------------------------------------ #
  trainer:
    max_epoch: 100
    gpuid: [0]
    clip_norm: 5
    start_scheduling: 1
    test_epochs: [20, 50, 100]
    precision: "fp16"
    grad_accumulation_steps: 4
    dummy_input:
      - shape: [1]
        dtype: long
      - shape: [1, 1, *var_sample_rate]
        dtype: float32

  checkpointer:
    save_interval: 5 # num epochs to save checkpoint
    save_best: true
    keep_last_n: null # null or int
    checkpoint_path: null # checkpoint to load or start from
